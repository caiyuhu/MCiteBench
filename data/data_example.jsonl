{"question_id": "27cea54636057f07daba34636ef1471dff674e139cb9c97058974f19f7101acd", "pdf_id": "67e2edb048c731ed4c87843ae8a048f4be355f16", "question_type": "explanation", "question": "How does GROD compare with the baseline NPOS in terms of OOD sample generation and superiority?", "answer": "To the best of our knowledge, the 'gold standard' for measuring the quality of synthetic OOD data has not been proposed. And compared to task performance in Table 2/3/4/5, GROD is superior. In Table 6 of our revised paper, we test changing the generating method to Gaussian and randomly uniform noise, which further shows the effect of GROD.", "evidence_keys": ["Table 2", "Table 6"], "evidence_contents": ["images/91a7fad5481d02a6218d71c696c003f5835d8a76084eeeb8879c939e9c6657ba.jpg", "images/a3c1da2d16f6a045c4d3834d5a895aa6dfe1a520f4b9a5de3656a745561c7d8b.jpg"], "evidence_modal": ["table"], "evidence_count": 2, "distractor_count": 3, "info_count": 5, "text_2_idx": {"In this section, we provide empirical evidence to validate the effectiveness of GROD across a range of real-world classification tasks and types of outliers, including comparison experiments with baselines on various NLP and CV tasks, and the ablation study of key parameters and modules in GROD. ": "1", "techniques. While many popular OOD detection algorithms are rigorously tested on image datasets, their effectiveness on text datasets does not exhibit marked superiority, as Table 4.2 illustrates. In addition, methods like ODIN (Liang et al., 2017) and G-ODIN (Hsu et al., 2020), which compute data gradients, necessitate floating-point number inputs. However, the tokenizer-encoded long integers used as input tokens create data format incompatibilities when attempting to use BERT and GPT-2 alongside ODIN or G-ODIN. Given their marginal performance on image datasets, these methods are excluded from text classification tasks. For the decoder-only GPT-2 model, some methods (Baseline, GEN) are compatible with both models using CLS tokens as features and without them, as they only require logits for processing. Others are only compatible with transformers with CLS tokens since they combine features and logits. We test two modes (with/without CLS token), labeled Method-C (with CLS) and Method-L (without CLS). As shown in Table 4.2, GROD consistently improves model performance across both image and text datasets and various OOD detection tasks, highlighting its versatility and broad applicability. ": "2"}, "idx_2_text": {"1": "In this section, we provide empirical evidence to validate the effectiveness of GROD across a range of real-world classification tasks and types of outliers, including comparison experiments with baselines on various NLP and CV tasks, and the ablation study of key parameters and modules in GROD. ", "2": "techniques. While many popular OOD detection algorithms are rigorously tested on image datasets, their effectiveness on text datasets does not exhibit marked superiority, as Table 4.2 illustrates. In addition, methods like ODIN (Liang et al., 2017) and G-ODIN (Hsu et al., 2020), which compute data gradients, necessitate floating-point number inputs. However, the tokenizer-encoded long integers used as input tokens create data format incompatibilities when attempting to use BERT and GPT-2 alongside ODIN or G-ODIN. Given their marginal performance on image datasets, these methods are excluded from text classification tasks. For the decoder-only GPT-2 model, some methods (Baseline, GEN) are compatible with both models using CLS tokens as features and without them, as they only require logits for processing. Others are only compatible with transformers with CLS tokens since they combine features and logits. We test two modes (with/without CLS token), labeled Method-C (with CLS) and Method-L (without CLS). As shown in Table 4.2, GROD consistently improves model performance across both image and text datasets and various OOD detection tasks, highlighting its versatility and broad applicability. "}, "image_2_idx": {}, "idx_2_image": {}, "table_2_idx": {"images/91a7fad5481d02a6218d71c696c003f5835d8a76084eeeb8879c939e9c6657ba.jpg": "2", "images/a3c1da2d16f6a045c4d3834d5a895aa6dfe1a520f4b9a5de3656a745561c7d8b.jpg": "6", "images/37ce7bca3475057496ed12ad81cd36fe0f6c5dde908f44fda2ed2fd559268a2c.jpg": "1"}, "idx_2_table": {"2": "images/91a7fad5481d02a6218d71c696c003f5835d8a76084eeeb8879c939e9c6657ba.jpg", "6": "images/a3c1da2d16f6a045c4d3834d5a895aa6dfe1a520f4b9a5de3656a745561c7d8b.jpg", "1": "images/37ce7bca3475057496ed12ad81cd36fe0f6c5dde908f44fda2ed2fd559268a2c.jpg"}, "meta_data": {}, "distractor_contents": ["In this section, we provide empirical evidence to validate the effectiveness of GROD across a range of real-world classification tasks and types of outliers, including comparison experiments with baselines on various NLP and CV tasks, and the ablation study of key parameters and modules in GROD. ", "techniques. While many popular OOD detection algorithms are rigorously tested on image datasets, their effectiveness on text datasets does not exhibit marked superiority, as Table 4.2 illustrates. In addition, methods like ODIN (Liang et al., 2017) and G-ODIN (Hsu et al., 2020), which compute data gradients, necessitate floating-point number inputs. However, the tokenizer-encoded long integers used as input tokens create data format incompatibilities when attempting to use BERT and GPT-2 alongside ODIN or G-ODIN. Given their marginal performance on image datasets, these methods are excluded from text classification tasks. For the decoder-only GPT-2 model, some methods (Baseline, GEN) are compatible with both models using CLS tokens as features and without them, as they only require logits for processing. Others are only compatible with transformers with CLS tokens since they combine features and logits. We test two modes (with/without CLS token), labeled Method-C (with CLS) and Method-L (without CLS). As shown in Table 4.2, GROD consistently improves model performance across both image and text datasets and various OOD detection tasks, highlighting its versatility and broad applicability. ", "images/37ce7bca3475057496ed12ad81cd36fe0f6c5dde908f44fda2ed2fd559268a2c.jpg"]}
{"question_id": "8dff87f10a4746a9de034a7b32e136a34b63e74e4207bc070488d09288f1f2ef", "pdf_id": "3116bda2b92bc73967c37ed846d9e4b814c14e2c", "question_type": "explanation", "question": "What evidence supports the claim that attention sinks can appear at any position within a sentence?", "answer": "We have provided an example of key visualization in Figure 1 (b). Additionally, we have randomly selected 10 examples in GSM8K and provided the positions of the four tokens with the highest attention scores.", "evidence_keys": ["Figure 1"], "evidence_contents": ["images/334e6d9e2309590d5b7fb463ae9b675bd3eb9b8744e6bd405347831d101ffc9b.jpg"], "evidence_modal": ["figure"], "evidence_count": 1, "distractor_count": 4, "info_count": 5, "text_2_idx": {"• We investigate the outlier channels of the KV Cache and identify that some tokens deviate from the previous assumptions.   \n• We introduce Sink-aware KV Cache Quantization (SinkQ), a simple yet effective method to dynamically identify and exclude these tokens during quantization, thus improving overall quantization accuracy.   \n• Our method achieves significant accuracy improvements under 2-bit quantization, yielding a 6.4× reduction in memory usage and a 2.3× increase in throughput, thereby enhancing model efficiency. ": "1", "Based on the insights above, we propose Sink-aware KV Cache Quantization (SinkQ). Figure 2 illustrates an overview of our method. SinkQ consists of two components: quantization and decoding. ": "2", "Figure 3: Experiments on throughput and memory: (a) Comparison of throughput (tokens/s) for different methods across different batch sizes on NVIDIA A800 80G. (b) Peak memory usage (including model weights and other components) at different batch sizes on NVIDIA A800 80G. (c) Peak memory usage (including model weights and other components) at different sequence lengths when batch size = 1 on NVIDIA A100 40G. SinkQ achieves a peak memory reduction of up to 6.4× and a throughput increase of 2.3×. ": "3"}, "idx_2_text": {"1": "• We investigate the outlier channels of the KV Cache and identify that some tokens deviate from the previous assumptions.   \n• We introduce Sink-aware KV Cache Quantization (SinkQ), a simple yet effective method to dynamically identify and exclude these tokens during quantization, thus improving overall quantization accuracy.   \n• Our method achieves significant accuracy improvements under 2-bit quantization, yielding a 6.4× reduction in memory usage and a 2.3× increase in throughput, thereby enhancing model efficiency. ", "2": "Based on the insights above, we propose Sink-aware KV Cache Quantization (SinkQ). Figure 2 illustrates an overview of our method. SinkQ consists of two components: quantization and decoding. ", "3": "Figure 3: Experiments on throughput and memory: (a) Comparison of throughput (tokens/s) for different methods across different batch sizes on NVIDIA A800 80G. (b) Peak memory usage (including model weights and other components) at different batch sizes on NVIDIA A800 80G. (c) Peak memory usage (including model weights and other components) at different sequence lengths when batch size = 1 on NVIDIA A100 40G. SinkQ achieves a peak memory reduction of up to 6.4× and a throughput increase of 2.3×. "}, "image_2_idx": {"images/334e6d9e2309590d5b7fb463ae9b675bd3eb9b8744e6bd405347831d101ffc9b.jpg": "1"}, "idx_2_image": {"1": "images/334e6d9e2309590d5b7fb463ae9b675bd3eb9b8744e6bd405347831d101ffc9b.jpg"}, "table_2_idx": {"images/87efe878a8dbd5aee20f519819ce19f94c90a870056f3c09e354642e4897559a.jpg": "3"}, "idx_2_table": {"3": "images/87efe878a8dbd5aee20f519819ce19f94c90a870056f3c09e354642e4897559a.jpg"}, "meta_data": {}, "distractor_contents": ["• We investigate the outlier channels of the KV Cache and identify that some tokens deviate from the previous assumptions.   \n• We introduce Sink-aware KV Cache Quantization (SinkQ), a simple yet effective method to dynamically identify and exclude these tokens during quantization, thus improving overall quantization accuracy.   \n• Our method achieves significant accuracy improvements under 2-bit quantization, yielding a 6.4× reduction in memory usage and a 2.3× increase in throughput, thereby enhancing model efficiency. ", "Figure 3: Experiments on throughput and memory: (a) Comparison of throughput (tokens/s) for different methods across different batch sizes on NVIDIA A800 80G. (b) Peak memory usage (including model weights and other components) at different batch sizes on NVIDIA A800 80G. (c) Peak memory usage (including model weights and other components) at different sequence lengths when batch size = 1 on NVIDIA A100 40G. SinkQ achieves a peak memory reduction of up to 6.4× and a throughput increase of 2.3×. ", "Based on the insights above, we propose Sink-aware KV Cache Quantization (SinkQ). Figure 2 illustrates an overview of our method. SinkQ consists of two components: quantization and decoding. ", "images/87efe878a8dbd5aee20f519819ce19f94c90a870056f3c09e354642e4897559a.jpg"]}
{"question_id": "f53063f963e44e2574a5cd7501bc1f7fd3aaca6636ab96b7abf7e28af6a7295a", "pdf_id": "10fba7417034e47a64749de88831c86e5fa4a3aa", "question_type": "locating", "question": "What do the generalisation error curves indicate about the ReLU and Sigmoidal activation functions?", "answer": "The Sigmoidal function has a more rapid decrease in generalisation error compared to the ReLU function.", "evidence_keys": ["Figure 1"], "evidence_contents": ["images/2fc98cbe2b75f8e0b99b91e9b65a32223914aaf619d7df9c745157c79870b2e2.jpg"], "evidence_modal": ["figure"], "evidence_count": 1, "distractor_count": 4, "info_count": 5, "text_2_idx": {"where c is a defined constant, τ = 1 is the learning time constant and k = 4s2 + λ2d2. Eq. 3 shows that k is the variable interacting with time (t) and as a consequence determines how quickly the network will learn. Three factors affect k fastening learning: 1. s the input-output correlation matrix singular value, 2. d the input correlation matrix singular value, and 3. λ = h2 −wwwwwwT which denotes the imbalance between the weights of the network. Notice that–as shown in Appendix A–λ is a conserved quantity and constant throughout training. Thus, given a dataset–which determine the s and d matrices–the only property which can promote faster learning in the network is to increase the imbalance parameter. For our experiments we whiten the input data xxx such that k = 4s2 + λ2 to remove one of the interactions within k. With the training dynamics of a singular value defined as in Eq. 3, we can formally define the escaping time as tˆ = t such that ω(t) = δ for a small δ ∈R. Similarly, we define the hitting time as t∗= t such that ω(∞) −ω(t) = δ for a small δ ∈R ": "1", "where η is the learning rate and y(t) is the target output from the teacher network for task t. ": "2", "Results are shown in Fig. 4, despite very similar levels of reconstruction loss, networks initialised with smaller gains improved disentanglement in the β-VAE network, as reflected in higher Disentanglement, Completeness, and Informativeness (DCI) scores (Eastwood & Williams, 2018). This result confirms that modulating the initialisation gain can either enhance or reduce the network’s disentanglement. Although the scope of these experiments is limited, they provide preliminary validation of our theoretical framework in more realistic contexts, encouraging further investigation into alternative initialisation schemes with varying levels of balance. Having investigated the role initialisation plays in promoting specialisation, we return to the original setting of Sec. 2 to understand the role of initialisation in governing network behaviour during continual learning. Specifically, we aim to revisit in the light these results two established forgetting profiles empirically observed in the continual learning literature: Namely, the Maslow’s Hammer profile, observed empirically first in Ramasesh et al. (2020), and the monotonic forgetting profile, more typically assumed and observed in Goodfellow et al. (2013). ": "3"}, "idx_2_text": {"1": "where c is a defined constant, τ = 1 is the learning time constant and k = 4s2 + λ2d2. Eq. 3 shows that k is the variable interacting with time (t) and as a consequence determines how quickly the network will learn. Three factors affect k fastening learning: 1. s the input-output correlation matrix singular value, 2. d the input correlation matrix singular value, and 3. λ = h2 −wwwwwwT which denotes the imbalance between the weights of the network. Notice that–as shown in Appendix A–λ is a conserved quantity and constant throughout training. Thus, given a dataset–which determine the s and d matrices–the only property which can promote faster learning in the network is to increase the imbalance parameter. For our experiments we whiten the input data xxx such that k = 4s2 + λ2 to remove one of the interactions within k. With the training dynamics of a singular value defined as in Eq. 3, we can formally define the escaping time as tˆ = t such that ω(t) = δ for a small δ ∈R. Similarly, we define the hitting time as t∗= t such that ω(∞) −ω(t) = δ for a small δ ∈R ", "2": "where η is the learning rate and y(t) is the target output from the teacher network for task t. ", "3": "Results are shown in Fig. 4, despite very similar levels of reconstruction loss, networks initialised with smaller gains improved disentanglement in the β-VAE network, as reflected in higher Disentanglement, Completeness, and Informativeness (DCI) scores (Eastwood & Williams, 2018). This result confirms that modulating the initialisation gain can either enhance or reduce the network’s disentanglement. Although the scope of these experiments is limited, they provide preliminary validation of our theoretical framework in more realistic contexts, encouraging further investigation into alternative initialisation schemes with varying levels of balance. Having investigated the role initialisation plays in promoting specialisation, we return to the original setting of Sec. 2 to understand the role of initialisation in governing network behaviour during continual learning. Specifically, we aim to revisit in the light these results two established forgetting profiles empirically observed in the continual learning literature: Namely, the Maslow’s Hammer profile, observed empirically first in Ramasesh et al. (2020), and the monotonic forgetting profile, more typically assumed and observed in Goodfellow et al. (2013). "}, "image_2_idx": {"images/2fc98cbe2b75f8e0b99b91e9b65a32223914aaf619d7df9c745157c79870b2e2.jpg": "1", "images/cb3e1335ec1cf8665ad00555231ca58b990659b6cf6a6b1c69abf5a014c00d7c.jpg": "5"}, "idx_2_image": {"1": "images/2fc98cbe2b75f8e0b99b91e9b65a32223914aaf619d7df9c745157c79870b2e2.jpg", "5": "images/cb3e1335ec1cf8665ad00555231ca58b990659b6cf6a6b1c69abf5a014c00d7c.jpg"}, "table_2_idx": {}, "idx_2_table": {}, "meta_data": {"Gold": "B", "A": "The ReLU function shows a higher generalisation error than the Sigmoidal function.", "B": "The Sigmoidal function has a more rapid decrease in generalisation error compared to the ReLU function.", "C": "Both functions have the same generalisation error over time.", "D": "The ReLU function does not change in generalisation error over time.", "page_idx": 3, "explanation": "The generalisation error curve for the Sigmoidal activation function decreases more rapidly, indicating better performance compared to the ReLU function, which has a slower decrease in generalisation error.", "img_caption": "Figure 1: Initialisation impacts specialisation. a) In the teacher-student setup a student network is trained with labels generated by a fixed teacher network. Previous work established a relationship between the activation function ϕ and the propensity for the student nodes to specialise to teacher nodes. However we show in this work that this is an overly simplistic description; other factors including student weight initialisations IW , Ih, parameterised by ΘW , Θh arguably play a stronger role. b) Generalisation error curves for two simulations of the teacher-student setup, one with a ReLU activation function and one with a scaled error activation function. ΘW and Θh are chosen to achieve a solution with ReLU that specialises—as indicated by sparser overlap matrices on the bottom right, and a scaled error function solution that does not specialise—as indicated by denser overlap matrices on the top right. A sparse (dense) Q matrix shows few (many) nodes are active, while a sparse (dense) R matrix shows student nodes are representing teacher nodes in a targeted (redundant) manner. Further details for the quantities described can be found in Sec. 4.", "itable_caption": null}, "distractor_contents": ["images/cb3e1335ec1cf8665ad00555231ca58b990659b6cf6a6b1c69abf5a014c00d7c.jpg", "Results are shown in Fig. 4, despite very similar levels of reconstruction loss, networks initialised with smaller gains improved disentanglement in the β-VAE network, as reflected in higher Disentanglement, Completeness, and Informativeness (DCI) scores (Eastwood & Williams, 2018). This result confirms that modulating the initialisation gain can either enhance or reduce the network’s disentanglement. Although the scope of these experiments is limited, they provide preliminary validation of our theoretical framework in more realistic contexts, encouraging further investigation into alternative initialisation schemes with varying levels of balance. Having investigated the role initialisation plays in promoting specialisation, we return to the original setting of Sec. 2 to understand the role of initialisation in governing network behaviour during continual learning. Specifically, we aim to revisit in the light these results two established forgetting profiles empirically observed in the continual learning literature: Namely, the Maslow’s Hammer profile, observed empirically first in Ramasesh et al. (2020), and the monotonic forgetting profile, more typically assumed and observed in Goodfellow et al. (2013). ", "where c is a defined constant, τ = 1 is the learning time constant and k = 4s2 + λ2d2. Eq. 3 shows that k is the variable interacting with time (t) and as a consequence determines how quickly the network will learn. Three factors affect k fastening learning: 1. s the input-output correlation matrix singular value, 2. d the input correlation matrix singular value, and 3. λ = h2 −wwwwwwT which denotes the imbalance between the weights of the network. Notice that–as shown in Appendix A–λ is a conserved quantity and constant throughout training. Thus, given a dataset–which determine the s and d matrices–the only property which can promote faster learning in the network is to increase the imbalance parameter. For our experiments we whiten the input data xxx such that k = 4s2 + λ2 to remove one of the interactions within k. With the training dynamics of a singular value defined as in Eq. 3, we can formally define the escaping time as tˆ = t such that ω(t) = δ for a small δ ∈R. Similarly, we define the hitting time as t∗= t such that ω(∞) −ω(t) = δ for a small δ ∈R ", "where η is the learning rate and y(t) is the target output from the teacher network for task t. "]}